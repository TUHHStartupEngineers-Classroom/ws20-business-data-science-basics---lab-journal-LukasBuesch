---
title: "Journal"
author: "Lukas Buesch"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
options(knitr.duplicate.label = "allow")
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

# Intro to the tidyverse
In the following you will find my R Code and the corresponding plots for the challenge in the chapter **Intro to the tidyverse**.



```{r challenge_1, fig.width=14, fig.height=7}
# Data Science at TUHH ------------------------------------------------------
# SALES ANALYSIS - Challenge ----

# 1.0 Load libraries ----
library(tidyverse)
library(readxl)
library(lubridate)


# 2.0 Importing Files ----

bikes_tbl      <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl  <- read_excel("DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")


# 3.0 Joining Data ----

# leftjoin the specified columns to the orderlines_tbl

bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))


# 5.0 Wrangling Data ----

# Split the location column

bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  
  # 5.1 Separate location name
  
  separate(col    = location,
           into   = c("city", "state"),
           sep    = ", ") %>%
  
  # 5.2 Add the total price (price * quantity) 
  # Add a column to a tibble that uses a formula-style calculation of other columns
  
  mutate(total.price = price * quantity) %>%
  
  # 5.3 Optional: Reorganize. Using select to grab or remove unnecessary columns
  # 5.3.1 by exact column name
  
  select(-...1, -gender) %>%
  
  # 5.3.2 by a pattern
  
  select(-ends_with(".id")) %>%
  
  # 5.3.3 Actually we need the column "order.id". Let's bind it back to the data
  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>% 
  
  # 5.3.4 You can reorder the data by selecting the columns in your desired order.
  
  select(order.id, contains("order"), contains("model"), contains("category"),
         price, quantity, total.price,
         everything()) %>%
  
  # 5.4 Rename columns -> dots to underscores
  
  rename(bikeshop = name) %>%
  set_names(names(.) %>% str_replace_all("\\.", "_"))

# 6.0 Business Insights ----
# 6.1 Sales by location (state) ----

# Step 1 - Manipulate

sales_by_location_tbl <- bike_orderlines_wrangled_tbl %>%
  
  # Select columns
  
  select(state, total_price) %>%
  
  # Grouping by location and summarizing sales
  
  group_by(state) %>% 
  summarize(sales = sum(total_price)) %>%
  
  # Optional: Add a column that turns the numbers into a currency format 
  # (makes it in the plot optically more appealing)
  # mutate(sales_text = scales::dollar(sales)) <- Works for dollar values
  
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

# Step 2 - Visualize

sales_by_location_tbl %>%
  
  # Setup canvas with the columns state (x-axis) and sales (y-axis)
  
  ggplot(aes(x = state, y = sales)) +
  
  # Geometries
  
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = sales_text)) + # Adding labels to the bars
  geom_smooth(method = "lm", se = FALSE) + # Adding a trendline
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # rotate x-labels
  
  # Formatting
  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. 
  
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by state",
    subtitle = "-",
    x = "", # Override defaults for x and y
    y = "Revenue"
  )


# 6.2 Sales by Year and Category 2 ----

# Step 1 - Manipulate
sales_by_location_year_tbl <- bike_orderlines_wrangled_tbl %>%
  
  # Select columns and add a year
  
  select(order_date, total_price, state) %>%
  mutate(year = year(order_date)) %>%
  
  # Group by and summarize year and state
  
  group_by(year, state) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() %>%
  
  # Format $ Text
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

# Step 2 - Visualize
sales_by_location_year_tbl %>%
  
  # Set up x, y, fill
  ggplot(aes(x = year, y = sales, fill = state)) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  # Facet
  facet_wrap(~ state) +
  
  #Trendline
  geom_smooth(method = "lm", se = FALSE) +
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year and state",
    subtitle = "-",
    fill = "states" # Changes the legend name
  )


```


# Data Acquivision
In the following you will find my R Code for the challenges in the chapter **Data Acquivision**.

## Challenge 1 - API
For this challenge I decided to call a weather API, namely `OpenWeather`, to get the current weather in Hamburg.
```{r chapter_2_challenge_1}
# API challenge ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.s
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(httr)


# 2.0 Get data  ----
# Get data from weather API  via function

weather_api <- function(city) {
  # path is city name
  API_key <- Sys.getenv("API_key")
  url <- modify_url(url = glue("https://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_key}"))
  resp <- GET(url)
  stop_for_status(resp) # automatically throws an error if a request did not succeed
}


weather_hamburg_raw <- weather_api(city = "Hamburg")

weather_hamburg <- fromJSON(rawToChar(weather_hamburg_raw$content))

weather_hamburg


```


## Challenge 2 - Webscrapping
I decided to analyze the website `https://www.rosebikes.de/`. Later I noticed that this Website uses sub categories for their bikes. I choosed the First search tree (mtb/trail-/-enduro/ground-control) for my analysis. The script saves the name (within the url) and the price.
```{r chapter_2_challenge_2}
# WEBSCRAPING ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(purrr)


# 1.1 COLLECT PRODUCT FAMILIES ----

url_home          <- "https://www.rosebikes.de/"

# Read in the HTML for the entire webpage
html_home         <- read_html(url_home)

# Web scrape the ids for the families
bike_family_tbl <- html_home %>%
  
  # Get the nodes for the families ...
  html_nodes(css = ".main-navigation-category-with-tiles__link") %>%
  # ...and extract the information of the id attribute
  html_attr("href") %>%
  
  # Remove the product families Gear and Outlet and Woman 
  # (because the female bikes are also listed with the others)
  discard(.p = ~stringr::str_detect(.x,"sale")) %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "family_class") %>%
  
  # Add a hashtag so we can get nodes of the categories by id (#)
  mutate(
    family_id = str_glue("#{family_class}")
  )

bike_family_tbl


# 1.2 COLLECT PRODUCT CATEGORIES ----

# Combine all Ids to one string to get all nodes at once
# (seperated by the OR operator ",")

family_id_css <- bike_family_tbl %>%
  
  pull(family_class) %>%
  
  stringr::str_c(collapse = ", ")


family_id_css



# 2.1 Get URL for each bike of the Product categories

# Create vector with all URLS for product classes

bike_category_url_tbl <- bike_family_tbl %>%
  
  # Convert vector to tibble
  select("position", "family_class") %>%
  
  # Add the domain, because we will get only the family_class
  mutate(
    url = glue("https://www.rosebikes.de{family_class}")
  ) %>%
  
  # delete multiple entries
  distinct(url)




# 2.1 Get URL for each bike of the Product categories

# select first bike category url
bike_category_url <- bike_category_url_tbl$url[1]

# Get the URLs for the bikes of the first category
html_bike_category  <- read_html(bike_category_url)
bike_sub_category_url_tbl        <- html_bike_category %>%
  
  # Get the 'a' nodes, which are hierarchally underneath 
  # the class productTile__contentWrapper
  html_nodes(css = ".catalog-category-bikes__picture-wrapper--left") %>%
  html_attr("href") %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "url") %>%

  # Add the domain, because we will get only the subdirectories

   mutate(url = glue("https://www.rosebikes.de{url}")) %>%
     
  # delete multiple entries
     
  distinct(url)


# search one Layer deeper

bike_sub_category_url <- bike_sub_category_url_tbl$url[1]

# Get the URLs for the bikes of the first subcategory

html_bike_sub_category  <- read_html(bike_sub_category_url)

bike_url_tbl        <- html_bike_sub_category %>%
  
  # Get the 'a' nodes, which are hierarchally underneath 
  # the class productTile__contentWrapper
  
  html_nodes(css = ".catalog-category-model__link") %>%
  html_attr("href") %>%
  
  # Convert vector to tibble
  
  enframe(name = "position", value = "url")



  # Add the full URL
  
bike_url_tbl$url <- paste("https://www.rosebikes.de", trimws(bike_url_tbl$url), sep = "")



  
  
# 2.1.2 Extract the descriptions (since we have retrieved the data already)
bike_url <- bike_url_tbl$url[1]
html_bike <- read_html(bike_url)

bike_desc_tbl <- html_bike %>%
  
  # Get the nodes in the meta tag where the attribute itemprop equals description
  html_nodes(".buybox__prize__wrapper > span") %>%
  
  # Extract the content of the attribute content
  html_attr("data-test") %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "price")





get_bike_data <- function(bike_url) {
  
  # Get the descriptions
  html_bike <- read_html(bike_url)
  bike_desc_tbl <- html_bike %>%
    html_nodes(".buybox__prize__wrapper > span") %>%
    html_attr("data-test") %>%
    enframe(name = "position", value = "price")
  
}





# Run the function with the first url to check if it is working
bike_url <- bike_url_tbl$url[1]
bike_data_tbl <- get_bike_data(bike_url)

bike_data_tbl



# 2.3.1a Map the function against all urls

# Extract the urls as a character vector
bike_url_vec <- bike_url_tbl %>% 
  pull(url)

# Run the function with every url as an argument
bike_data_lst <- map(bike_url_vec, get_bike_data)

# Merge the list into a tibble
bike_data_tbl <- bind_rows(bike_data_lst)


name_with_price = bind_cols(bike_data_tbl$price,bike_url_tbl$url)

name_with_price





```







# Data Wrangling
In the following you will find my R Code for the challenges in the chapter **Data Wrangling**. My machine is able to execute each chunk on its own but rendering the website would require a better memory handling or a lot more RAM. Therefore I decided to work with the smaller data set.

## Challenge 1 - Patent Dominance
```{r chapter_4_challenge_1}
# challenge 1 ----


# import libraries ----
library(vroom)
library(data.table)
library(tidyverse)
library(lubridate)


# import data ----


import_assignee <- function(){
  
  col_types <- list(
    id = col_character(),
    type = col_double(),
    #name_first = col_character(),
    #name_last = col_character(),
    organization = col_character()
  )
  
  assignee_tbl <- vroom(
    file       = "Data_wrangling/assignee_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(assignee_tbl)
  setDT(assignee_tbl)
  
}

import_patent <- function(){
  
  col_types <- list(
    id = col_skip(),
    #type = col_skip(),
    #number = col_character(),
    #country = col_skip(),
    date = col_date("%Y-%m-%d"),
    #abstract = col_skip(),
    #title = col_skip(),
    #kind = col_skip(),
    num_claims = col_skip()#,
    #filename = col_skip(),
    #withdrawn = col_skip()
  )
  
  patent_tbl <- vroom(
    file       = "Data_wrangling/patent_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(patent_tbl)
  setDT(patent_tbl)
}


import_patent_assignee <- function(){
  
  col_types <- list(
    patent_id = col_character(),
    assignee_id = col_character()#,
    #location_id = col_character()
  )
  
  patent_assignee_tbl <- vroom(
    file       = "Data_wrangling/patent_assignee_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(patent_assignee_tbl)
  setDT(patent_assignee_tbl)
}


import_uspc <- function(){
  
  col_types <- list(
    # uuid = col_character(),
    patent_id = col_character(),
    mainclass_id = col_character(),
    #subclass_id = col_character(),
    sequence = col_double()
  )
  
  uspc_tbl <- vroom(
    file       = "Data_wrangling/uspc_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(uspc_tbl)
  setDT(uspc_tbl)
}


# Import tables

assignee_1_tbl <- import_assignee()
patent_assignee_1_tbl <- import_patent_assignee()

# rename id to assignee_id
setnames(assignee_1_tbl,"id","assignee_id")

# join tables by id
combined_data_1 <- merge(x = patent_assignee_1_tbl, y = assignee_1_tbl, 
                       by    = "assignee_id", 
                       all.x = TRUE, 
                       all.y = FALSE)



# in type: integer for country (2 - US Company or Corporation)

US_comp_tbl <- combined_data_1[type == "2"]

# reorder after appearance ----

ranking_tbl <- US_comp_tbl[,.(count = .N), by = organization][
  order(count, decreasing = TRUE)]

head(ranking_tbl, 10)


```


## Challenge 2 - Recent patent activity
```{r chapter_4_challenge_2}


# challenge 2 ----

# import libraries ----
library(vroom)
library(data.table)
library(tidyverse)
library(lubridate)


# import data ----


import_assignee <- function(){
  
  col_types <- list(
    id = col_character(),
    type = col_double(),
    #name_first = col_character(),
    #name_last = col_character(),
    organization = col_character()
  )
  
  assignee_tbl <- vroom(
    file       = "Data_wrangling/assignee_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(assignee_tbl)
  setDT(assignee_tbl)
  
}

import_patent <- function(){
  
  col_types <- list(
    id = col_character(),
    #type = col_skip(),
    #number = col_character(),
    #country = col_skip(),
    date = col_date("%Y-%m-%d"),
    #abstract = col_skip(),
    #title = col_skip(),
    #kind = col_skip(),
    num_claims = col_skip()#,
    #filename = col_skip(),
    #withdrawn = col_skip()
  )
  
  patent_tbl <- vroom(
    file       = "Data_wrangling/patent_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(patent_tbl)
  setDT(patent_tbl)
}


import_patent_assignee <- function(){
  
  col_types <- list(
    patent_id = col_character(),
    assignee_id = col_character()#,
    #location_id = col_character()
  )
  
  patent_assignee_tbl <- vroom(
    file       = "Data_wrangling/patent_assignee_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(patent_assignee_tbl)
  setDT(patent_assignee_tbl)
}


import_uspc <- function(){
  
  col_types <- list(
    # uuid = col_character(),
    patent_id = col_character(),
    mainclass_id = col_character(),
    #subclass_id = col_character(),
    sequence = col_double()
  )
  
  uspc_tbl <- vroom(
    file       = "Data_wrangling/uspc_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(uspc_tbl)
  setDT(uspc_tbl)
}

# import tables

assignee_2_tbl <- import_assignee()
patent_assignee_2_tbl <- import_patent_assignee()
patent_2_tbl <- import_patent()


# rename id to assignee_id
setnames(assignee_2_tbl,"id","assignee_id")

# rename number to patent_id
setnames(patent_2_tbl,"id","patent_id")

# join tables by id
combined_data_2_0 <- merge(x = patent_assignee_2_tbl, y = assignee_2_tbl, 
                           by    = "assignee_id", 
                           all.x = TRUE, 
                           all.y = FALSE)
combined_data_2_1 <- merge(x = combined_data_2_0, y = patent_2_tbl, 
                           by = "patent_id",
                           all.x = TRUE, 
                           all.y = FALSE)


# build 3 columns out of date 

temp <- combined_data_2_1 %>% mutate_at(vars(date), funs(year, month, day)) 

# search in month column

US_comp_month_1_raw_tbl <- temp[month == "1"]

# filter NA

US_comp_month_1_tbl <- US_comp_month_1_raw_tbl[organization != "NA"]

# reorder after appearance ----

ranking_tbl <- US_comp_month_1_tbl[,.(count = .N), by = organization][
  order(count, decreasing = TRUE)]

head(ranking_tbl, 10)


```


## Challenge 3 - Innovation in Tech
```{r chapter_4_challenge_3}


# challenge 3 ----

# import libraries ----
library(vroom)
library(data.table)
library(tidyverse)
library(lubridate)


# import data ----


import_assignee <- function(){
  
  col_types <- list(
    id = col_character(),
    type = col_double(),
    #name_first = col_character(),
    #name_last = col_character(),
    organization = col_character()
  )
  
  assignee_tbl <- vroom(
    file       = "Data_wrangling/assignee_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(assignee_tbl)
  setDT(assignee_tbl)
  
}

import_patent <- function(){
  
  col_types <- list(
    id = col_skip(),
    #type = col_skip(),
    #number = col_character(),
    #country = col_skip(),
    date = col_date("%Y-%m-%d"),
    #abstract = col_skip(),
    #title = col_skip(),
    #kind = col_skip(),
    num_claims = col_skip()#,
    #filename = col_skip(),
    #withdrawn = col_skip()
  )
  
  patent_tbl <- vroom(
    file       = "Data_wrangling/patent_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(patent_tbl)
  setDT(patent_tbl)
}


import_patent_assignee <- function(){
  
  col_types <- list(
    patent_id = col_character(),
    assignee_id = col_character()#,
    #location_id = col_character()
  )
  
  patent_assignee_tbl <- vroom(
    file       = "Data_wrangling/patent_assignee_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(patent_assignee_tbl)
  setDT(patent_assignee_tbl)
}


import_uspc <- function(){
  
  col_types <- list(
    # uuid = col_character(),
    patent_id = col_character(),
    mainclass_id = col_character(),
    #subclass_id = col_character(),
    sequence = col_double()
  )
  
  uspc_tbl <- vroom(
    file       = "Data_wrangling/uspc_small.tsv", 
    delim      = "\t", 
    col_types  = col_types,
    na         = c("", "NA", "NULL")
  )
  class(uspc_tbl)
  setDT(uspc_tbl)
}

# import tables

assignee_3_tbl <- import_assignee()
patent_assignee_3_tbl <- import_patent_assignee()
uspc_3_tbl <- import_uspc()


# rename id to assignee_id
setnames(assignee_3_tbl,"id","assignee_id")

# join tables by id
combined_data_3_1 <- merge(x = patent_assignee_3_tbl, y = assignee_3_tbl, 
                         by    = "assignee_id", 
                         all.x = TRUE, 
                         all.y = FALSE)

# join tables by patent_id
combined_data_3_2 <- merge(x = combined_data_3_1, y = uspc_3_tbl, 
                         by    = "patent_id", 
                         all.x = TRUE, 
                         all.y = FALSE)



# reorder after appearance ----

ranking_raw_tbl <- combined_data_3_2[,.(count = .N), by = organization][
  order(count, decreasing = TRUE)]

# filter NA

ranking_tbl <- ranking_raw_tbl[organization != "NA"]

head(ranking_tbl, 10)



# get patents of first 10 companies ----

# reduce data (only first 10 companies)
 
patents_first_10_tbl <- combined_data_3_2[organization %in% c(ranking_tbl[1,organization] 
                                                          , ranking_tbl[2,organization] 
                                                          , ranking_tbl[3,organization] 
                                                          , ranking_tbl[4,organization]
                                                          , ranking_tbl[5,organization]
                                                          , ranking_tbl[6,organization]
                                                          , ranking_tbl[7,organization]
                                                          , ranking_tbl[8,organization]
                                                          , ranking_tbl[9,organization]
                                                          , ranking_tbl[10,organization])]



# reorder after appearance (USPTO mainclasses) ----

ranking_raw_tbl <- patents_first_10_tbl[,.(count = .N), by = mainclass_id][
  order(count, decreasing = TRUE)]

# filter NA

ranking_tbl <- ranking_raw_tbl[mainclass_id != "NA"]

head(ranking_tbl, 5)


```



# Data Visualization
In the following you will find my R Code for the challenges in the chapter **Data Visualization**.

## Challenge 1 - time course of the cumulative Covid-19 cases

```{r}
# Chapter 05 - Challenge 1 ----

# import libraries ----

library(tidyverse)
library(lubridate)

# import data ----

covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")



covid_data_tbl <- covid_data_tbl%>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " "))


# Data Manipulation x axis - month, y axis - cumulative cases
cumulative_cases_tbl <- covid_data_tbl %>%
  
  # change date format
  
  mutate(date = dmy(dateRep)) %>%
  
  arrange(date) %>%
  

  # Filter for nations of interest and year
  
  filter(countriesAndTerritories %in% c("Germany","Spain", "France", "United_Kingdom", "United_States_of_America")) %>%
  
  filter(year == "2020") %>%
  
  # grouping
  
  group_by(countriesAndTerritories) %>%
  mutate(cumulative_cases = cumsum(cases)) %>%
  ungroup()





# Plot ----

cumulative_cases_tbl %>%

  # Canvas
  
  ggplot(aes(date, cumulative_cases), color = countriesAndTerritories) +
  
  # Geoms
  
  
  
  geom_line(aes(x     = date,
                y     = cumulative_cases,
                color = countriesAndTerritories)) + 
  
  scale_x_date(breaks = "1 month", minor_breaks = "1 month", date_labels = "%B") +
  scale_y_continuous(labels = scales::dollar_format(prefix = "", suffix = "M")) +
  
  
  # geom_label(aes(,
  #   y = max(cumulative_cases),
  #   label = cumulative_cases,
  #   fill = factor(countriesAndTerritories),
  #   #colour = "black",
  #   #fontface = "bold",
  #   vjust = "inward",
  #   hjust = "inward",
  #   data = cumulative_cases_tbl$cumulative_cases[1])) +
  
  

  
  
  theme_light() +
  
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    plot.title = element_text(face = "bold"),
    plot.caption = element_text(face = "bold.italic"),
    plot.background = element_blank(),
    axis.title = element_text(face = "bold")
    ) +
  
  labs(
    title = "COVID-19 confirmed cases worldwide",
    subtitle = "------------",
    x = "Year 2020",
    y = "Cumulative Cases",
    color = "Continent / Country" # Legend text
  )

```

## Challenge 2 - world mortality map

```{r chapter5_cahllenge2}
# Chapter 05 - Challenge 2 ----

# import libraries ----

library(tidyverse)
library(ggplot2)
library(maps)

# import data ----

covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")



covid_data_tbl <- covid_data_tbl%>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
    
  ))


# data manipulation ----

mortality_tbl <- covid_data_tbl %>%
  
  # Select relevant columns
  select(countriesAndTerritories, deaths, popData2019, cases) %>%
  
  # grouping
  
  group_by(countriesAndTerritories) %>%
  summarize(population_2019 = mean(popData2019), deaths_sum = sum(deaths)) %>%
  mutate(`Mortality Rate [%]`  = 100 * deaths_sum / population_2019) %>%
  ungroup()


  
  
# join world map with mortality
  
world <- map_data("world")

world <- left_join(world, mortality_tbl, by = c("region" = "countriesAndTerritories"))

world <- select(world, -c("population_2019","deaths_sum"))





# plotting ---


ggplot() + 

geom_polygon(data = world,
             aes(x=long,
             y = lat,
             fill = `Mortality Rate [%]`,
             group = group)) + 
  
coord_fixed(1.3) + 
  
scale_fill_gradient(low='#EC4440',
                    high='#2F142C') +
  
theme(axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()) +
  
theme(axis.title.y=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
  
labs(title = "Confirmed COVIS-19 deaths relativ to the size of the population",
     subtitle = "More then 1.2 Million confirmed COVID-19 deaths worldwide")

```










